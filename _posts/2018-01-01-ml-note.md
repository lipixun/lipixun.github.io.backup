---
layout: post
title: "机器学习笔记 - 1x1CNN"
category: 机器学习笔记
description: "1x1CNN是什么鬼？"
date: 2018-01-01
tags: [paper]
comments: true
---

在学习Google Inception网络的时候发现了一个很有趣的网络结构，就是**1 x 1**的卷积网络层。过去我们应用卷积网络一般是选择一个适当大小的窗口，譬如**3 x 3**，**5 x 5**这样的，意义也很好理解，相当于我们让模型一次性扫描一个区域把这个区域的特征抽取出来，形成一个更高级的特征抽象。那么这个**1 x 1**的卷积层就很有趣了，因为输入和输出的窗口大小没有变化且模型并不能学习到一个区域局部的特征，这样的一层卷积网络的意义是什么尼？

这是一个很有趣的问题，我搜集了一些相关的文章，以及自己的理解以下是我认为**1 x 1**卷积网络的意义：

* 一种降维方式。虽然**1 x 1**卷积网络的输入和输出的窗口大小是一样的，但是可以改变**channel**的数量，在**Google Inception**网络中，为了让网络层数更深的同时降低由此带来的训练成本的提升，实际上是通过在输入更深层卷积网络之前加入了**1 x 1**卷积网络来给**channel**降维。
* 更低的过拟合概率。很明显，**1 x 1**卷积网络相对于更大的卷积网络在过拟合的问题上会处理的更好。（当然相应的，拟合的程度就会下降，因此其应当和其他卷积网络一起使用）
* 输入数据的欠采样。通过结合**1 x 1**卷积网络和**stride/pooling**方法可以实现对输入数据的欠采样，某种程度上同样是一种的防止过拟合的手段。
* 顺序无关计算。**1 x 1**卷积网络和**MLP**在数学上是等价的（tf上也可以用reshape + matmul来模拟1x1 CNN），其计算的结果和输入数据的顺序是无关的，对于一些离散性质的输入可能会有帮助。
* 多重映射（我自己的命名）。这一点我是在读**Google Tensor2Tensor**的代码的时候发现的，其在计算**Attention**之前通过一个**1 x 1**的卷积神经网络实现了对同一个序列的多重映射（Projection）。关于这个多重映射的意义以及如何在**Attention**中起到作用我会在之后写一篇有关**Attention**的文章里介绍。我自己的试验表明，这个映射非常重要。

如果大家对**1 x 1 CNN**还有新的见解欢迎留言讨论。
